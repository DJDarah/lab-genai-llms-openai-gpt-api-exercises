{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAB GenAI - LLMs - OpenAI GPT API Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Basic Conversation\n",
    "**Exercise:** Create a simple chatbot that can answer basic questions about a given topic (e.g., history, technology).  \n",
    "**Parameters to explore:** `temperature`, `max_tokens`, `top_p`, `frequency_penalty`, `presence_penalty`, `n`, `stop`.\n",
    "\n",
    "Comment what happen when you change the parameters \n",
    "(read documentation!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "# Function to interact with the chatbot\n",
    "def chat_with_bot(prompt, temperature=0.7, max_tokens=100, top_p=1.0, frequency_penalty=0.0, presence_penalty=0.0, n=1, stop=None):\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",  # Adjust to the model you are using\n",
    "        messages=[{\"role\": \"system\", \"content\": \"You are a helpful chatbot answering questions about history.\"},\n",
    "                  {\"role\": \"user\", \"content\": prompt}],\n",
    "       temperature=temperature,  # Controls randomness (lower = more consistent results)\n",
    "        max_tokens=max_tokens,  # Limits response length (ensures short, direct classification)\n",
    "        top_p=top_p,  # Controls probability distribution for word choices (lower = more strict)\n",
    "        frequency_penalty=frequency_penalty,  # Reduces or increases word repetition\n",
    "        presence_penalty=presence_penalty,  # Encourages introducing new words\n",
    "        n=n,  # Number of responses to generate (can check consistency of responses)\n",
    "        stop=stop # Defines a stopping condition (e.g., stop=[\"User:\"] makes the bot stop when it sees \"User:\")\n",
    "    )\n",
    "    \n",
    "    return response[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "# Example usage\n",
    "question = \"Who was Julius Caesar?\"\n",
    "answer = chat_with_bot(question)\n",
    "print(answer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Summarization\n",
    "**Exercise:** Write a script that takes a long text input and summarizes it into a few sentences.  \n",
    "**Parameters to explore:** `temperature`, `max_tokens`, `top_p`, `frequency_penalty`, `presence_penalty`, `best_of`, `logprobs`.\n",
    "\n",
    "Comment what happen when you change the parameters \n",
    "(read documentation!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "# Function to summarize text\n",
    "def summarize_text(text, temperature=0.7, max_tokens=100, top_p=1.0, frequency_penalty=0.0, presence_penalty=0.0, best_of=1, logprobs=None):\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[{\"role\": \"system\", \"content\": \"You are a helpful assistant that summarizes long texts.\"},\n",
    "                  {\"role\": \"user\", \"content\": f\"Summarize the following text in a few sentences:\\n\\n{text}\"}],\n",
    "       temperature=temperature,  # Controls randomness (lower = more consistent results)\n",
    "        max_tokens=max_tokens,  # Limits response length (ensures short, direct classification)\n",
    "        top_p=top_p,  # Controls probability distribution for word choices (lower = more strict)\n",
    "        frequency_penalty=frequency_penalty,  # Reduces or increases word repetition\n",
    "        presence_penalty=presence_penalty,  # Encourages introducing new words\n",
    "        best_of=best_of, # Generates multiple completions and selects the best one (higher values improve quality but increase cost)\n",
    "        logprobs=logprobs  # Returns probability scores for each token (useful for analyzing confidence)\n",
    "    )\n",
    "\n",
    "    return response[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "# Example usage\n",
    "long_text = \"\"\"\n",
    "Julius Caesar was a Roman general, statesman, and dictator. He played a key role in the fall of the Roman Republic and the rise of the Roman Empire. \n",
    "His military campaigns, notably in Gaul, expanded Roman territory significantly. Despite his popularity among soldiers and the lower class, \n",
    "his increasing power alarmed the Senate. In 44 BC, he was assassinated by a group of senators, including his close friend Brutus. \n",
    "His death led to a series of civil wars, culminating in the rise of Augustus as Romeâ€™s first emperor.\n",
    "\"\"\"\n",
    "\n",
    "summary = summarize_text(long_text)\n",
    "print(summary)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Translation\n",
    "**Exercise:** Develop a tool that translates text from one language to another using the API.  \n",
    "**Parameters to explore:** `temperature`, `max_tokens`, `top_p`, `frequency_penalty`, `presence_penalty`, `echo`, `logit_bias`.\n",
    "\n",
    "Comment what happen when you change the parameters \n",
    "(read documentation!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "# Function to translate text\n",
    "def translate_text(text, target_language=\"French\", temperature=0.7, max_tokens=100, top_p=1.0, \n",
    "                   frequency_penalty=0.0, presence_penalty=0.0, echo=False, logit_bias=None):\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[{\"role\": \"system\", \"content\": f\"You are a translator who translates text into {target_language}.\"},\n",
    "                  {\"role\": \"user\", \"content\": f\"Translate the following text into {target_language}:\\n\\n{text}\"}],\n",
    "        temperature=temperature,  # Controls randomness (lower = more consistent results)\n",
    "        max_tokens=max_tokens,  # Limits response length (ensures short, direct classification)\n",
    "        top_p=top_p,  # Controls probability distribution for word choices (lower = more strict)\n",
    "        frequency_penalty=frequency_penalty,  # Reduces or increases word repetition\n",
    "        presence_penalty=presence_penalty,  # Encourages introducing new words\n",
    "        echo=echo,  # If True, includes the original input in the response along with the output\n",
    "        logit_bias=logit_bias  # Adjusts token probability; can encourage or discourage specific words in the output\n",
    "    )\n",
    "        \n",
    "    return response[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "# Example usage\n",
    "text_to_translate = \"Hello, how are you?\"\n",
    "translated_text = translate_text(text_to_translate, target_language=\"Spanish\")\n",
    "print(translated_text)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Sentiment Analysis\n",
    "**Exercise:** Implement a sentiment analysis tool that determines the sentiment of a given text (positive, negative, neutral).  \n",
    "**Parameters to explore:** `temperature`, `max_tokens`, `top_p`, `frequency_penalty`, `presence_penalty`, `n`, `logprobs`.\n",
    "\n",
    "Comment what happen when you change the parameters \n",
    "(read documentation!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "# Function to analyze sentiment\n",
    "def analyze_sentiment(text, temperature=0.3, max_tokens=10, top_p=0.9, \n",
    "                      frequency_penalty=0.0, presence_penalty=0.0, n=1, logprobs=5):\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[{\"role\": \"system\", \"content\": \"You are a sentiment analysis tool that classifies text as Positive, Negative, or Neutral.\"},\n",
    "                  {\"role\": \"user\", \"content\": f\"Analyze the sentiment of the following text and respond with 'Positive', 'Negative', or 'Neutral' only:\\n\\n{text}\"}],\n",
    "        temperature=temperature,  # Controls randomness (lower = more consistent results)\n",
    "        max_tokens=max_tokens,  # Limits response length (ensures short, direct classification)\n",
    "        top_p=top_p,  # Controls probability distribution for word choices (lower = more strict)\n",
    "        frequency_penalty=frequency_penalty,  # Reduces or increases word repetition\n",
    "        presence_penalty=presence_penalty,  # Encourages introducing new words\n",
    "        n=n,  # Number of responses to generate (can check consistency of responses)\n",
    "        logprobs=logprobs  # Returns probability scores for each token (useful for analyzing confidence)\n",
    "    )\n",
    "\n",
    "    return response[\"choices\"][0][\"message\"][\"content\"].strip()\n",
    "\n",
    "# Example usage\n",
    "text_to_analyze = \"I had a fantastic day at work today!\"\n",
    "sentiment = analyze_sentiment(text_to_analyze)\n",
    "print(f\"Sentiment: {sentiment}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Text Completion\n",
    "**Exercise:** Create a text completion application that generates text based on an initial prompt.  \n",
    "**Parameters to explore:** `temperature`, `max_tokens`, `top_p`, `frequency_penalty`, `presence_penalty`, `stop`, `best_of`.\n",
    "\n",
    "Comment what happen when you change the parameters \n",
    "(read documentation!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "# Function to generate text completion\n",
    "def complete_text(prompt, temperature=0.7, max_tokens=100, top_p=1.0, \n",
    "                   frequency_penalty=0.0, presence_penalty=0.0, stop=None, best_of=1):\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[{\"role\": \"system\", \"content\": \"You are a creative text generator.\"},\n",
    "                  {\"role\": \"user\", \"content\": f\"{prompt}\"}],\n",
    "        temperature=temperature,  # Controls randomness (higher = more creative, lower = more predictable)\n",
    "        max_tokens=max_tokens,  # Limits response length (higher = longer completions)\n",
    "        top_p=top_p,  # Controls probability distribution for word choices (lower = more focused)\n",
    "        frequency_penalty=frequency_penalty,  # Discourages word repetition\n",
    "        presence_penalty=presence_penalty,  # Encourages introducing new words or concepts\n",
    "        stop=stop,  # Defines stopping criteria (e.g., stops at a certain word or phrase)\n",
    "        best_of=best_of  # Generates multiple completions and selects the best one (higher = better quality but increased cost)\n",
    "    )\n",
    "\n",
    "    return response[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "# Example usage\n",
    "prompt_text = \"Once upon a time in a distant land,\"\n",
    "completed_text = complete_text(prompt_text)\n",
    "print(completed_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BONUS: Google Vertex AI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Basic Conversation\n",
    "**Exercise:** Create a basic chatbot using Google Vertex AI to answer questions about a given topic.  \n",
    "**Parameters to explore:** `temperature`, `max_output_tokens`, `top_p`, `frequency_penalty`, `presence_penalty`, `n`, `stop`.\n",
    "\n",
    "Comment what happen when you change the parameters \n",
    "(read documentation!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "\n",
    "# Function to interact with the chatbot\n",
    "def chat_with_vertex(prompt, project_id, location=\"us-central1\", \n",
    "                     temperature=0.7, max_output_tokens=100, top_p=1.0, \n",
    "                     frequency_penalty=0.0, presence_penalty=0.0, n=1, stop=None):\n",
    "    \n",
    "    # Initialize the Vertex AI model\n",
    "    model = aiplatform.generation.TextGenerationModel.from_pretrained(\"text-bison@001\")\n",
    "    \n",
    "    response = model.predict(\n",
    "        prompt=prompt,\n",
    "        temperature=temperature,  # Controls randomness (higher = more creative, lower = more predictable)\n",
    "        max_output_tokens=max_output_tokens,  # Limits response length (higher = longer answers)\n",
    "        top_p=top_p,  # Controls probability distribution for word choices (lower = safer responses)\n",
    "        frequency_penalty=frequency_penalty,  # Reduces repetition of words\n",
    "        presence_penalty=presence_penalty,  # Encourages introducing new words and concepts\n",
    "        n=n,  # Number of responses generated (higher = multiple answers)\n",
    "        stop=stop  # Defines stopping conditions (e.g., stops after a certain phrase)\n",
    "    )\n",
    "    \n",
    "    return response.text  # Returns the generated response\n",
    "\n",
    "# Example usage\n",
    "project_id = \"your-google-cloud-project-id\"\n",
    "question = \"Who was Julius Caesar?\"\n",
    "answer = chat_with_vertex(question, project_id)\n",
    "print(answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Summarization\n",
    "**Exercise:** Develop a script that summarizes long text inputs using Google Vertex AI.  \n",
    "**Parameters to explore:** `temperature`, `max_output_tokens`, `top_p`, `frequency_penalty`, `presence_penalty`, `best_of`, `logprobs`.\n",
    "\n",
    "Comment what happen when you change the parameters \n",
    "(read documentation!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "\n",
    "# Function to summarize text\n",
    "def summarize_text(text, project_id, location=\"us-central1\", \n",
    "                   temperature=0.5, max_output_tokens=150, top_p=0.9, \n",
    "                   frequency_penalty=0.0, presence_penalty=0.0, \n",
    "                   best_of=1, logprobs=None):\n",
    "    \n",
    "    # Initialize the Vertex AI model\n",
    "    model = aiplatform.generation.TextGenerationModel.from_pretrained(\"text-bison@001\")\n",
    "    \n",
    "    response = model.predict(\n",
    "        prompt=f\"Summarize the following text:\\n\\n{text}\",\n",
    "        temperature=temperature,  # Controls randomness (lower = more structured summary)\n",
    "        max_output_tokens=max_output_tokens,  # Limits summary length\n",
    "        top_p=top_p,  # Controls probability distribution (lower = safer summarization)\n",
    "        frequency_penalty=frequency_penalty,  # Reduces repeated words\n",
    "        presence_penalty=presence_penalty,  # Encourages introducing new words\n",
    "        best_of=best_of,  # Generates multiple summaries and selects the best one\n",
    "        logprobs=logprobs  # Returns probability scores of generated words\n",
    "    )\n",
    "    \n",
    "    return response.text  # Returns the summarized text\n",
    "\n",
    "# Example usage\n",
    "project_id = \"your-google-cloud-project-id\"\n",
    "long_text = \"\"\"\n",
    "Julius Caesar was a Roman general, statesman, and dictator. He played a key role in the fall of the Roman Republic and the rise of the Roman Empire. \n",
    "His military campaigns, notably in Gaul, expanded Roman territory significantly. Despite his popularity among soldiers and the lower class, \n",
    "his increasing power alarmed the Senate. In 44 BC, he was assassinated by a group of senators, including his close friend Brutus. \n",
    "His death led to a series of civil wars, culminating in the rise of Augustus as Romeâ€™s first emperor.\n",
    "\"\"\"\n",
    "summary = summarize_text(long_text, project_id)\n",
    "print(summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Translation\n",
    "**Exercise:** Create a tool that translates text from one language to another using Google Vertex AI.  \n",
    "**Parameters to explore:** `temperature`, `max_output_tokens`, `top_p`, `frequency_penalty`, `presence_penalty`, `echo`, `logit_bias`.\n",
    "\n",
    "Comment what happen when you change the parameters \n",
    "(read documentation!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "\n",
    "# Function to translate text\n",
    "def translate_text(text, target_language=\"French\", project_id=\"your-google-cloud-project-id\", location=\"us-central1\",\n",
    "                   temperature=0.7, max_output_tokens=100, top_p=1.0, \n",
    "                   frequency_penalty=0.0, presence_penalty=0.0, echo=False, logit_bias=None):\n",
    "    \n",
    "    # Initialize the Vertex AI model\n",
    "    model = aiplatform.generation.TextGenerationModel.from_pretrained(\"text-bison@001\")\n",
    "    \n",
    "    response = model.predict(\n",
    "        prompt=f\"Translate the following text into {target_language}:\\n\\n{text}\",\n",
    "        temperature=temperature,  # Controls randomness (higher = more diverse, lower = more literal)\n",
    "        max_output_tokens=max_output_tokens,  # Limits translation length\n",
    "        top_p=top_p,  # Controls probability mass (lower = safer translations)\n",
    "        frequency_penalty=frequency_penalty,  # Reduces word repetition\n",
    "        presence_penalty=presence_penalty,  # Encourages introducing new words\n",
    "        echo=echo,  # If True, repeats the input along with the translation\n",
    "        logit_bias=logit_bias  # Can be used to influence token probabilities (e.g., favor certain words)\n",
    "    )\n",
    "    \n",
    "    return response.text  # Returns the translated text\n",
    "\n",
    "# Example usage\n",
    "text_to_translate = \"Hello, how are you?\"\n",
    "translated_text = translate_text(text_to_translate, target_language=\"Spanish\")\n",
    "print(translated_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Sentiment Analysis\n",
    "**Exercise:** Implement a sentiment analysis tool using Google Vertex AI to determine the sentiment of a given text.  \n",
    "**Parameters to explore:** `temperature`, `max_output_tokens`, `top_p`, `frequency_penalty`, `presence_penalty`, `n`, `logprobs`.\n",
    "\n",
    "Comment what happen when you change the parameters \n",
    "(read documentation!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "\n",
    "# Function to analyze sentiment\n",
    "def analyze_sentiment(text, project_id=\"your-google-cloud-project-id\", location=\"us-central1\", \n",
    "                      temperature=0.3, max_output_tokens=10, top_p=0.9, \n",
    "                      frequency_penalty=0.0, presence_penalty=0.0, n=1, logprobs=None):\n",
    "    \n",
    "    # Initialize the Vertex AI model\n",
    "    model = aiplatform.generation.TextGenerationModel.from_pretrained(\"text-bison@001\")\n",
    "    \n",
    "    response = model.predict(\n",
    "        prompt=f\"Analyze the sentiment of the following text and classify it as 'Positive', 'Negative', or 'Neutral':\\n\\n{text}\",\n",
    "        temperature=temperature,  # Controls randomness (lower = more consistent classification)\n",
    "        max_output_tokens=max_output_tokens,  # Limits response length to keep it concise\n",
    "        top_p=top_p,  # Controls probability distribution for word choices (lower = more focused)\n",
    "        frequency_penalty=frequency_penalty,  # Reduces repetition of words\n",
    "        presence_penalty=presence_penalty,  # Encourages introducing new words and concepts\n",
    "        n=n,  # Number of sentiment responses generated (higher = multiple variations)\n",
    "        logprobs=logprobs  # Returns probability scores for each predicted sentiment category\n",
    "    )\n",
    "    \n",
    "    return response.text.strip()  # Returns the sentiment classification\n",
    "\n",
    "# Example usage\n",
    "text_to_analyze = \"I absolutely love this product! It's fantastic!\"\n",
    "sentiment = analyze_sentiment(text_to_analyze)\n",
    "print(f\"Sentiment: {sentiment}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Text Completion\n",
    "**Exercise:** Develop a text completion application using Google Vertex AI to generate text based on an initial prompt.  \n",
    "**Parameters to explore:** `temperature`, `max_output_tokens`, `top_p`, `frequency_penalty`, `presence_penalty`, `stop`, `best_of`.\n",
    "\n",
    "Comment what happen when you change the parameters \n",
    "(read documentation!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "\n",
    "# Function to generate text completion\n",
    "def complete_text(prompt, project_id=\"your-google-cloud-project-id\", location=\"us-central1\", \n",
    "                   temperature=0.7, max_output_tokens=100, top_p=1.0, \n",
    "                   frequency_penalty=0.0, presence_penalty=0.0, stop=None, best_of=1):\n",
    "    \n",
    "    # Initialize the Vertex AI model\n",
    "    model = aiplatform.generation.TextGenerationModel.from_pretrained(\"text-bison@001\")\n",
    "    \n",
    "    response = model.predict(\n",
    "        prompt=prompt,\n",
    "        temperature=temperature,  # Controls randomness (higher = more creative, lower = more predictable)\n",
    "        max_output_tokens=max_output_tokens,  # Limits response length (higher = longer completions)\n",
    "        top_p=top_p,  # Controls probability distribution for word choices (lower = more focused)\n",
    "        frequency_penalty=frequency_penalty,  # Discourages word repetition\n",
    "        presence_penalty=presence_penalty,  # Encourages introducing new words or concepts\n",
    "        stop=stop,  # Defines stopping criteria (e.g., stops at a certain word or phrase)\n",
    "        best_of=best_of  # Generates multiple completions and selects the best one (higher = better quality but increased cost)\n",
    "    )\n",
    "    \n",
    "    return response.text.strip()  # Returns the completed text\n",
    "\n",
    "# Example usage\n",
    "prompt_text = \"Once upon a time in a distant land,\"\n",
    "completed_text = complete_text(prompt_text)\n",
    "print(completed_text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
